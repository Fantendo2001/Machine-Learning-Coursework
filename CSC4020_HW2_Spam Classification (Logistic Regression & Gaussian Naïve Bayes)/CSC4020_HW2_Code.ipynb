{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC4020 HW2 Programming\n",
    "### @117020119 Jiang Jingxin \n",
    "## Spam Classification (Gaussian Naive Bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import io \n",
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_spam_data():\n",
    "    raw_data = io.loadmat('spamData.mat')\n",
    "    Xtrain = pd.DataFrame(raw_data['Xtrain'])\n",
    "    ytrain = pd.Series(np.hstack(raw_data['ytrain']))\n",
    "    Xtest = pd.DataFrame(raw_data['Xtest'])\n",
    "    ytest = pd.Series(np.hstack(raw_data['ytest']))\n",
    "    return Xtrain, ytrain, Xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, ytrain, Xtest, ytest = read_spam_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 8.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss Function**：$ f({\\bf w})=NLL({\\bf w})+\\lambda {\\bf w}^T{\\bf w}= - \\sum_{i=1}^N[y_i\\log\\mu_i+(1-y_i)\\log(1-\\mu_i)]+ \\lambda\\sum_{i=0}^k w_i^2$\n",
    "\n",
    "**Gradient**: $g({\\bf w}) = X^T({\\bf \\mu}-{\\bf y})+2\\lambda {\\bf w}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 923,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, λ, lr = 1,alpha=0.5, beta=0.5, max_iter=10000, tol = 1e-3, \n",
    "                 add_intercept = True, transform = None):\n",
    "        self.lr = lr # learning rate\n",
    "        self.alpha = alpha # step size\n",
    "        self.beta = beta # step size shrinking factor \n",
    "        self.max_iter = max_iter # maximal interations\n",
    "        self.λ = λ # regularization term\n",
    "        self.iters = 0 # number of iterations\n",
    "        self.tol = tol # stopping criteria of backtracking line search\n",
    "        self.add_intercept = add_intercept # add intercept to X by default \n",
    "        self.transform = transform # type of transformation: stnd, log or binary\n",
    "        \n",
    "    def _transform(self, X):\n",
    "        if self.transform == \"stnd\": # standardize\n",
    "            X = (X-X.mean())/X.std()\n",
    "        elif self.transform == \"log\": # logarithmize\n",
    "            X = np.log(X+0.1)\n",
    "        elif self.transform == \"binary\": #binarize\n",
    "            X = (X>0).astype(np.int32)\n",
    "        if self.add_intercept:\n",
    "            X = np.hstack([np.ones((X.shape[0],1)), X])\n",
    "        return X\n",
    "    \n",
    "    def _sigmoid(self, a):\n",
    "        return 1 / (1 + np.exp(-a))\n",
    "    \n",
    "    def _loss(self, y, μ, w):\n",
    "        return (sum(-np.log(μ[y==1]))+sum(-np.log(1-μ[y==0])) + self.λ*sum(w**2)) / y.size\n",
    "    \n",
    "    def _gradient(self, X, y, μ, w):\n",
    "        return (X.T@(μ-y)) / y.size + 2*self.λ*w / y.size\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.converged = False\n",
    "        \n",
    "        X = self._transform(X)\n",
    "        n, k = X.shape\n",
    "        \n",
    "        '''\n",
    "        gradient descent with backtracking line search\n",
    "        '''\n",
    "        prev_w = self.w = np.zeros(k)\n",
    "        μ = self._sigmoid(X@self.w)\n",
    "        prev_loss = self.loss = self._loss(y, μ, self.w)\n",
    "        gradient = self._gradient(X, y, μ, self.w)\n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "            norm_gradient = np.sqrt(sum(gradient**2))\n",
    "            if (norm_gradient < self.tol):\n",
    "                self.converged = True\n",
    "                break  \n",
    "            \n",
    "            self.iters += 1\n",
    "            t = self.lr\n",
    "            self.w = prev_w - t * gradient\n",
    "            μ = self._sigmoid(X@self.w)                       \n",
    "            self.loss = self._loss(y, μ, self.w)\n",
    "            \n",
    "            while (self.loss > prev_loss - self.alpha*t*norm_gradient**2):\n",
    "                t = t*self.beta;\n",
    "                self.w = prev_w - t*gradient\n",
    "                μ = self._sigmoid(X@self.w)\n",
    "                self.loss = self._loss(y, μ, self.w) \n",
    "                \n",
    "            prev_w = self.w\n",
    "            prev_loss = self.loss\n",
    "            gradient = self._gradient(X, y, μ, self.w)\n",
    "        \n",
    "    def predict_prob(self, X):\n",
    "        X = self._transform(X)\n",
    "        return self._sigmoid(X@self.w)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return (self.predict_prob(X)>0.5).astype(np.int32)\n",
    "    \n",
    "    def predict_error(self,X,y):\n",
    "        return sum(self.predict(X) != y)/y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "'''\n",
    "lambda parameter tuning using cross validation\n",
    "'''\n",
    "def CV(X, y, lambdas = 10**np.linspace(-2,1,10), transform = None, k = 10, seed = None):\n",
    "    np.random.seed(seed)\n",
    "    errors = np.zeros_like(lambdas)\n",
    "    \n",
    "    idx = np.arange(y.size) \n",
    "    np.random.shuffle(idx) \n",
    "    fold = np.array_split(idx,k) # split shuffled index into k folds\n",
    "\n",
    "    # cross validation using k folds\n",
    "    for i in tqdm(range(k)):\n",
    "        for l, lam in enumerate(lambdas):\n",
    "            test_idx = fold[i]\n",
    "            train_idx = np.setdiff1d(idx, test_idx)\n",
    "            mod = LogisticRegression(lam,transform = transform)\n",
    "            mod.fit(X.loc[train_idx],y[train_idx])\n",
    "            errors[l] += mod.predict_error(X.loc[test_idx],y[test_idx])\n",
    "    errors /= k\n",
    "    Lam = lambdas[np.argmin(errors)]\n",
    "    return Lam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:17<00:00, 19.76s/it]\n"
     ]
    }
   ],
   "source": [
    "lam_stnd = CV(Xtrain,ytrain,transform=\"stnd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 927,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9306977288832496"
      ]
     },
     "execution_count": 927,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lam_stnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 908,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfLR_stnd = LogisticRegression(λ=lam_stnd, transform = \"stnd\")\n",
    "clfLR_stnd.fit(Xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **training error**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07765089722675367"
      ]
     },
     "execution_count": 895,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfLR_stnd.predict_error(Xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **test error**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.087890625"
      ]
     },
     "execution_count": 896,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfLR_stnd.predict_error(Xtest,ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Logarithmize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [05:51<00:00, 35.12s/it]\n"
     ]
    }
   ],
   "source": [
    "lam_log = CV(Xtrain,ytrain,transform=\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 922,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1544346900318834"
      ]
     },
     "execution_count": 922,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lam_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 915,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfLR_log = LogisticRegression(λ=lam_log,transform = \"log\")\n",
    "clfLR_log.fit(Xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **training error**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.052528548123980424"
      ]
     },
     "execution_count": 898,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfLR_log.predict_error(Xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **test error**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.057291666666666664"
      ]
     },
     "execution_count": 899,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfLR_log.predict_error(Xtest,ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Binarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:42<00:00,  4.25s/it]\n"
     ]
    }
   ],
   "source": [
    "lam_bin = CV(Xtrain,ytrain,transform=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 928,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46415888336127786"
      ]
     },
     "execution_count": 928,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lam_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = LogisticRegression(λ=lam_bin,transform = \"binary\")\n",
    "mod.fit(Xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **training error**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06394779771615008"
      ]
     },
     "execution_count": 905,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod.predict_error(Xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **test error**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07356770833333333"
      ]
     },
     "execution_count": 906,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod.predict_error(Xtest,ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 8.2 Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. NaiveBayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{aligned}h({\\bf x}^n)&=\\arg\\max_c\\hat P({\\cal C}_c)\\hat P({\\bf x}^n|{\\cal C}_c)=\\arg\\max_c \\left(\\log \\hat P({\\cal C}_c)+ \\log \\hat P({\\bf x}^n|{\\cal C}_c)\\right)\\\\&=\\arg\\max_c \\left[\\log\\dfrac{N_c}{N}+\\sum_{i=1}^k \\left(x_{ni}\\log\\mu_{ic}+(1-x_{ni})\\log(1-\\mu_{ic})\\right)\\right]\\end{aligned}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self, pseudo = 1):\n",
    "        self.pseudo = pseudo\n",
    "        \n",
    "    def _binarize(self, X):\n",
    "        return (X>0).astype(np.int32)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X = self._binarize(X)\n",
    "        n, k = X.shape\n",
    "        C = len(np.unique(y)) # number of classes\n",
    "        self.theta = np.zeros([C,k]) # mean of each feature per class\n",
    "        self.prior = np.zeros(C)\n",
    "        for c in range(C):\n",
    "            self.theta[c] = (X[y==c].sum()+self.pseudo) / (sum(y==c)+self.pseudo*C)\n",
    "            self.prior[c] = sum(y==c) / n\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = self._binarize(X)\n",
    "        log_prior = np.log(self.prior)\n",
    "        log_post = X@np.log(self.theta.T) + (1-X)@np.log(1-self.theta.T) + log_prior\n",
    "        return log_post.idxmax(axis=1)\n",
    "    \n",
    "    def predict_error(self, X, y):\n",
    "        return sum(self.predict(X) != y)/y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfNB = NaiveBayes()\n",
    "clfNB.fit(Xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **training error**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11256117455138662"
      ]
     },
     "execution_count": 870,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfNB.predict_error(Xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **test error**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11002604166666667"
      ]
     },
     "execution_count": 871,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfNB.predict_error(Xtest,ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under **MLE**: $\\hat \\mu_{ck} = \\dfrac{1}{N_c}\\sum_{i=1}^Nx_{ik}{\\mathbb 1}_{y_i=c},\\ {\\hat \\sigma}_{ck}^2=\\dfrac{1}{N_c}\\sum_{i=1}^N(x_{ik}-\\hat\\mu_{ck})^2{\\mathbb 1}_{y_i=c}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 917,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNB:\n",
    "    def __init__(self, transform = None):\n",
    "        self.transform = transform\n",
    "        \n",
    "    def _transform(self, X):\n",
    "        if self.transform == \"stnd\":\n",
    "            return (X-X.mean())/X.std()\n",
    "        elif self.transform == \"log\":\n",
    "            return np.log(X+0.1)   \n",
    "        return X\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        X = self._transform(X)\n",
    "        n, k = X.shape\n",
    "        self.C = len(np.unique(y)) # number of class\n",
    "        self.theta = np.zeros([self.C,k]) # mean of each feature per class\n",
    "        self.sigma = np.zeros([self.C,k]) # variance of each feature per class\n",
    "        self.prior = np.zeros(self.C)\n",
    "        for c in range(self.C):\n",
    "            self.theta[c] = X[y==c].sum() / sum(y==c)\n",
    "            self.sigma[c] = X[y==c].var(ddof=0)\n",
    "            self.prior[c] = sum(y==c) / n\n",
    "        \n",
    "    def predict(self,X):\n",
    "        X = self._transform(X)\n",
    "        log_prior = np.log(self.prior)\n",
    "        log_post = np.zeros([self.C,X.shape[0]])\n",
    "        for i in range(self.C):\n",
    "            log_post[i] = multivariate_normal.logpdf(X, self.theta[i], np.diag(self.sigma[i]))\n",
    "        log_post =  log_post.T + log_prior\n",
    "        return np.argmax(log_post,axis=1)\n",
    "        \n",
    "    def predict_error(self,X,y):\n",
    "        return sum(self.predict(X) != y)/y.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.i) Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfGNB_stnd = GaussianNB(transform = \"stnd\")\n",
    "clfGNB_stnd.fit(Xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **training error**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17585644371941273"
      ]
     },
     "execution_count": 825,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfGNB_stnd.predict_error(Xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **test error**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18880208333333334"
      ]
     },
     "execution_count": 826,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfGNB_stnd.predict_error(Xtest,ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.ii) Logarithmize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfGNB_log = GaussianNB(transform = \"log\")\n",
    "clfGNB_log.fit(Xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **training error**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1634584013050571"
      ]
     },
     "execution_count": 821,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfGNB_log.predict_error(Xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **test error**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18098958333333334"
      ]
     },
     "execution_count": 820,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfGNB_log.predict_error(Xtest,ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('virtual': venv)",
   "language": "python",
   "name": "python361064bitvirtualvenv17b58f63dd9043ed85793e7f8be03798"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
